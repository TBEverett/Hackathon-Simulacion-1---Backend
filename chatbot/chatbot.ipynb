{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icksir/miniconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt# specify GPU \n",
    "from torchinfo import summary\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hola</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cómo estás</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>como estas</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holaaaa</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>holap</td>\n",
       "      <td>saludo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text   label\n",
       "0        Hola  saludo\n",
       "1  Cómo estás  saludo\n",
       "2  como estas  saludo\n",
       "3     holaaaa  saludo\n",
       "4       holap  saludo"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"category.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "saludo       9\n",
       "despedida    8\n",
       "equipo       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    0.409091\n",
       "0    0.363636\n",
       "1    0.227273\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label']) # check class distribution\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_labels = df['text'], df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "pd.Series(seq_len).hist(bins = 10)# Based on the histogram we are selecting the max len as 8\n",
    "\n",
    "max_seq_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", \"timpal0l/mdeberta-v3-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = qa_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler#define a batch size\n",
    "batch_size = 8# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)# DataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):   \n",
    "    def __init__(self, qa_model, out_features):      \n",
    "        super(BERT_Arch, self).__init__()       \n",
    "\n",
    "        self.bert = qa_model.model.base_model\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()       # dense layer       \n",
    "        self.fc1 = nn.Linear(768, 512)              \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, out_features)\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)       #define the forward pass\n",
    "\n",
    "    def forward(self, sent_id, mask):      #pass the inputs to the model  \n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "   \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                                            Trainable                 Param #\n",
       "===================================================================================================================\n",
       "BERT_Arch                                                         Partial                   --\n",
       "├─DebertaV2Model: 1-1                                             False                     --\n",
       "│    └─DebertaV2Embeddings: 2-1                                   False                     --\n",
       "│    │    └─Embedding: 3-1                                        False                     (192,768,000)\n",
       "│    │    └─LayerNorm: 3-2                                        False                     (1,536)\n",
       "│    │    └─StableDropout: 3-3                                    --                        --\n",
       "│    └─DebertaV2Encoder: 2-2                                      False                     --\n",
       "│    │    └─ModuleList: 3-4                                       False                     (85,054,464)\n",
       "│    │    └─Embedding: 3-5                                        False                     (393,216)\n",
       "│    │    └─LayerNorm: 3-6                                        False                     (1,536)\n",
       "├─Dropout: 1-2                                                    --                        --\n",
       "├─ReLU: 1-3                                                       --                        --\n",
       "├─Linear: 1-4                                                     True                      393,728\n",
       "├─Linear: 1-5                                                     True                      131,328\n",
       "├─Linear: 1-6                                                     True                      771\n",
       "├─LogSoftmax: 1-7                                                 --                        --\n",
       "===================================================================================================================\n",
       "Total params: 278,744,579\n",
       "Trainable params: 525,827\n",
       "Non-trainable params: 278,218,752\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in qa_model.model.parameters():\n",
    "      param.requires_grad = False\n",
    "      model = BERT_Arch(qa_model, 3) # push the model to GPU\n",
    "\n",
    "model = model.to(device)\n",
    "summary(model, col_names=(\"trainable\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icksir/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91666667 1.46666667 0.81481481]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)# loss function\n",
    "cross_entropy = nn.NLLLoss(weight=weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]# number of training epochs\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))   \n",
    "\n",
    "      # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch] \n",
    "        sent_id, mask, labels = batch    # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)    # compute the loss between actual and predicted values\n",
    "        \n",
    "        loss = cross_entropy(preds, labels)    # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()    # backward pass to calculate the gradients\n",
    "        loss.backward()    # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # update parameters\n",
    "        optimizer.step()    # clear calculated gradients\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "    # We are not using learning rate scheduler as of now\n",
    "    # lr_sch.step()    # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()    # append the model predictions\n",
    "        total_preds.append(preds)# compute the training loss of the epoch\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)#returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 1.101\n",
      "Epoch: 1 | Training Loss: 1.069\n",
      "Epoch: 2 | Training Loss: 1.047\n",
      "Epoch: 3 | Training Loss: 0.970\n",
      "Epoch: 4 | Training Loss: 0.959\n",
      "Epoch: 5 | Training Loss: 0.778\n",
      "Epoch: 6 | Training Loss: 0.673\n",
      "Epoch: 7 | Training Loss: 0.750\n",
      "Epoch: 8 | Training Loss: 0.711\n",
      "Epoch: 9 | Training Loss: 0.608\n",
      "Epoch: 10 | Training Loss: 0.651\n",
      "Epoch: 11 | Training Loss: 0.674\n",
      "Epoch: 12 | Training Loss: 0.469\n",
      "Epoch: 13 | Training Loss: 0.622\n",
      "Epoch: 14 | Training Loss: 0.375\n",
      "Epoch: 15 | Training Loss: 0.322\n",
      "Epoch: 16 | Training Loss: 0.504\n",
      "Epoch: 17 | Training Loss: 0.633\n",
      "Epoch: 18 | Training Loss: 0.446\n",
      "Epoch: 19 | Training Loss: 0.504\n",
      "Epoch: 20 | Training Loss: 0.655\n",
      "Epoch: 21 | Training Loss: 0.599\n",
      "Epoch: 22 | Training Loss: 0.486\n",
      "Epoch: 23 | Training Loss: 0.349\n",
      "Epoch: 24 | Training Loss: 0.465\n",
      "Epoch: 25 | Training Loss: 0.861\n",
      "Epoch: 26 | Training Loss: 0.522\n",
      "Epoch: 27 | Training Loss: 0.472\n",
      "Epoch: 28 | Training Loss: 0.631\n",
      "Epoch: 29 | Training Loss: 0.551\n",
      "Epoch: 30 | Training Loss: 0.402\n",
      "Epoch: 31 | Training Loss: 0.292\n",
      "Epoch: 32 | Training Loss: 0.557\n",
      "Epoch: 33 | Training Loss: 0.344\n",
      "Epoch: 34 | Training Loss: 0.244\n",
      "Epoch: 35 | Training Loss: 0.569\n",
      "Epoch: 36 | Training Loss: 0.315\n",
      "Epoch: 37 | Training Loss: 0.435\n",
      "Epoch: 38 | Training Loss: 0.338\n",
      "Epoch: 39 | Training Loss: 0.229\n",
      "Epoch: 40 | Training Loss: 0.281\n",
      "Epoch: 41 | Training Loss: 0.277\n",
      "Epoch: 42 | Training Loss: 0.299\n",
      "Epoch: 43 | Training Loss: 0.327\n",
      "Epoch: 44 | Training Loss: 0.256\n",
      "Epoch: 45 | Training Loss: 0.162\n",
      "Epoch: 46 | Training Loss: 0.300\n",
      "Epoch: 47 | Training Loss: 0.321\n",
      "Epoch: 48 | Training Loss: 0.404\n",
      "Epoch: 49 | Training Loss: 0.331\n",
      "Epoch: 50 | Training Loss: 0.573\n",
      "Epoch: 51 | Training Loss: 0.211\n",
      "Epoch: 52 | Training Loss: 0.249\n",
      "Epoch: 53 | Training Loss: 0.138\n",
      "Epoch: 54 | Training Loss: 0.230\n",
      "Epoch: 55 | Training Loss: 0.066\n",
      "Epoch: 56 | Training Loss: 0.185\n",
      "Epoch: 57 | Training Loss: 0.213\n",
      "Epoch: 58 | Training Loss: 0.376\n",
      "Epoch: 59 | Training Loss: 0.243\n",
      "Epoch: 60 | Training Loss: 0.134\n",
      "Epoch: 61 | Training Loss: 0.407\n",
      "Epoch: 62 | Training Loss: 0.763\n",
      "Epoch: 63 | Training Loss: 0.514\n",
      "Epoch: 64 | Training Loss: 0.493\n",
      "Epoch: 65 | Training Loss: 0.536\n",
      "Epoch: 66 | Training Loss: 0.316\n",
      "Epoch: 67 | Training Loss: 0.391\n",
      "Epoch: 68 | Training Loss: 0.232\n",
      "Epoch: 69 | Training Loss: 0.689\n",
      "Epoch: 70 | Training Loss: 0.129\n",
      "Epoch: 71 | Training Loss: 0.601\n",
      "Epoch: 72 | Training Loss: 0.258\n",
      "Epoch: 73 | Training Loss: 0.080\n",
      "Epoch: 74 | Training Loss: 0.334\n",
      "Epoch: 75 | Training Loss: 0.137\n",
      "Epoch: 76 | Training Loss: 0.264\n",
      "Epoch: 77 | Training Loss: 0.229\n",
      "Epoch: 78 | Training Loss: 0.249\n",
      "Epoch: 79 | Training Loss: 0.145\n",
      "Epoch: 80 | Training Loss: 0.305\n",
      "Epoch: 81 | Training Loss: 0.108\n",
      "Epoch: 82 | Training Loss: 0.439\n",
      "Epoch: 83 | Training Loss: 0.460\n",
      "Epoch: 84 | Training Loss: 0.396\n",
      "Epoch: 85 | Training Loss: 0.356\n",
      "Epoch: 86 | Training Loss: 0.271\n",
      "Epoch: 87 | Training Loss: 0.293\n",
      "Epoch: 88 | Training Loss: 0.117\n",
      "Epoch: 89 | Training Loss: 0.196\n",
      "Epoch: 90 | Training Loss: 0.364\n",
      "Epoch: 91 | Training Loss: 0.679\n",
      "Epoch: 92 | Training Loss: 0.252\n",
      "Epoch: 93 | Training Loss: 0.366\n",
      "Epoch: 94 | Training Loss: 0.116\n",
      "Epoch: 95 | Training Loss: 0.213\n",
      "Epoch: 96 | Training Loss: 0.621\n",
      "Epoch: 97 | Training Loss: 0.083\n",
      "Epoch: 98 | Training Loss: 0.256\n",
      "Epoch: 99 | Training Loss: 0.093\n",
      "Epoch: 100 | Training Loss: 0.568\n",
      "Epoch: 101 | Training Loss: 0.456\n",
      "Epoch: 102 | Training Loss: 0.211\n",
      "Epoch: 103 | Training Loss: 0.341\n",
      "Epoch: 104 | Training Loss: 0.073\n",
      "Epoch: 105 | Training Loss: 0.130\n",
      "Epoch: 106 | Training Loss: 0.200\n",
      "Epoch: 107 | Training Loss: 0.269\n",
      "Epoch: 108 | Training Loss: 0.468\n",
      "Epoch: 109 | Training Loss: 0.567\n",
      "Epoch: 110 | Training Loss: 0.239\n",
      "Epoch: 111 | Training Loss: 0.333\n",
      "Epoch: 112 | Training Loss: 0.189\n",
      "Epoch: 113 | Training Loss: 0.456\n",
      "Epoch: 114 | Training Loss: 0.290\n",
      "Epoch: 115 | Training Loss: 0.254\n",
      "Epoch: 116 | Training Loss: 0.047\n",
      "Epoch: 117 | Training Loss: 0.062\n",
      "Epoch: 118 | Training Loss: 0.444\n",
      "Epoch: 119 | Training Loss: 0.241\n",
      "Epoch: 120 | Training Loss: 0.273\n",
      "Epoch: 121 | Training Loss: 0.276\n",
      "Epoch: 122 | Training Loss: 0.539\n",
      "Epoch: 123 | Training Loss: 0.162\n",
      "Epoch: 124 | Training Loss: 0.090\n",
      "Epoch: 125 | Training Loss: 0.372\n",
      "Epoch: 126 | Training Loss: 0.233\n",
      "Epoch: 127 | Training Loss: 0.510\n",
      "Epoch: 128 | Training Loss: 0.041\n",
      "Epoch: 129 | Training Loss: 0.126\n",
      "Epoch: 130 | Training Loss: 0.081\n",
      "Epoch: 131 | Training Loss: 0.513\n",
      "Epoch: 132 | Training Loss: 0.344\n",
      "Epoch: 133 | Training Loss: 0.444\n",
      "Epoch: 134 | Training Loss: 0.030\n",
      "Epoch: 135 | Training Loss: 0.126\n",
      "Epoch: 136 | Training Loss: 0.126\n",
      "Epoch: 137 | Training Loss: 0.103\n",
      "Epoch: 138 | Training Loss: 0.220\n",
      "Epoch: 139 | Training Loss: 0.121\n",
      "Epoch: 140 | Training Loss: 0.208\n",
      "Epoch: 141 | Training Loss: 0.035\n",
      "Epoch: 142 | Training Loss: 0.129\n",
      "Epoch: 143 | Training Loss: 0.286\n",
      "Epoch: 144 | Training Loss: 0.218\n",
      "Epoch: 145 | Training Loss: 0.169\n",
      "Epoch: 146 | Training Loss: 0.289\n",
      "Epoch: 147 | Training Loss: 0.182\n",
      "Epoch: 148 | Training Loss: 0.142\n",
      "Epoch: 149 | Training Loss: 0.279\n",
      "Epoch: 150 | Training Loss: 0.315\n",
      "Epoch: 151 | Training Loss: 0.214\n",
      "Epoch: 152 | Training Loss: 0.458\n",
      "Epoch: 153 | Training Loss: 0.267\n",
      "Epoch: 154 | Training Loss: 0.405\n",
      "Epoch: 155 | Training Loss: 0.120\n",
      "Epoch: 156 | Training Loss: 0.082\n",
      "Epoch: 157 | Training Loss: 0.237\n",
      "Epoch: 158 | Training Loss: 0.596\n",
      "Epoch: 159 | Training Loss: 0.098\n",
      "Epoch: 160 | Training Loss: 0.315\n",
      "Epoch: 161 | Training Loss: 0.131\n",
      "Epoch: 162 | Training Loss: 0.311\n",
      "Epoch: 163 | Training Loss: 0.537\n",
      "Epoch: 164 | Training Loss: 0.189\n",
      "Epoch: 165 | Training Loss: 0.158\n",
      "Epoch: 166 | Training Loss: 0.104\n",
      "Epoch: 167 | Training Loss: 0.192\n",
      "Epoch: 168 | Training Loss: 0.229\n",
      "Epoch: 169 | Training Loss: 0.191\n",
      "Epoch: 170 | Training Loss: 0.278\n",
      "Epoch: 171 | Training Loss: 0.106\n",
      "Epoch: 172 | Training Loss: 0.477\n",
      "Epoch: 173 | Training Loss: 0.142\n",
      "Epoch: 174 | Training Loss: 0.357\n",
      "Epoch: 175 | Training Loss: 0.053\n",
      "Epoch: 176 | Training Loss: 0.118\n",
      "Epoch: 177 | Training Loss: 0.135\n",
      "Epoch: 178 | Training Loss: 0.112\n",
      "Epoch: 179 | Training Loss: 0.159\n",
      "Epoch: 180 | Training Loss: 0.152\n",
      "Epoch: 181 | Training Loss: 0.159\n",
      "Epoch: 182 | Training Loss: 0.271\n",
      "Epoch: 183 | Training Loss: 0.063\n",
      "Epoch: 184 | Training Loss: 0.144\n",
      "Epoch: 185 | Training Loss: 0.463\n",
      "Epoch: 186 | Training Loss: 0.014\n",
      "Epoch: 187 | Training Loss: 0.178\n",
      "Epoch: 188 | Training Loss: 0.074\n",
      "Epoch: 189 | Training Loss: 0.189\n",
      "Epoch: 190 | Training Loss: 0.224\n",
      "Epoch: 191 | Training Loss: 0.116\n",
      "Epoch: 192 | Training Loss: 0.657\n",
      "Epoch: 193 | Training Loss: 0.343\n",
      "Epoch: 194 | Training Loss: 0.500\n",
      "Epoch: 195 | Training Loss: 0.089\n",
      "Epoch: 196 | Training Loss: 0.136\n",
      "Epoch: 197 | Training Loss: 0.356\n",
      "Epoch: 198 | Training Loss: 0.080\n",
      "Epoch: 199 | Training Loss: 0.338\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "         \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f'Epoch: {epoch} | Training Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"intents\": [\n",
    "    {\"tag\": \"saludo\", \"responses\": [\"Hola!\", \"Hola\", \"Hola :)\"]},\n",
    "    {\"tag\": \"despedida\", \"responses\": [\"Adios\", \"Hasta pronto\"]},\n",
    "    {\"tag\": \"equipo\", \"responses\": [\"Somos Mining Track\"]},\n",
    "]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(str):\n",
    "    str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
    "    test_text = [str]\n",
    "    model.eval()\n",
    "    \n",
    "    tokens_test_data = tokenizer(\n",
    "    test_text,\n",
    "    max_length = max_seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    "    ) \n",
    "    test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
    "    \n",
    "    preds = None \n",
    "    with torch.no_grad():\n",
    "        preds = model(test_seq.to(device), test_mask.to(device)) \n",
    "\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "    print(\"predicciones:\", preds)\n",
    "\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "\n",
    "    print(\"indice:\", preds)\n",
    "\n",
    "    print(\"Intent Identified: \", le.inverse_transform(preds)[0])\n",
    "    return le.inverse_transform(preds)[0]\n",
    "\n",
    "def get_response(message): \n",
    "    intent = get_prediction(message)\n",
    "    result = \"\"\n",
    "    intent = re.sub(r'[^a-zA-Z]+', '', intent)\n",
    "\n",
    "    for i in data['intents']: \n",
    "        if i[\"tag\"] == intent and result == '':\n",
    "            result = random.choice(i[\"responses\"])\n",
    "        \n",
    "    print(f\"Response : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicciones: [[-2.1944022e-04 -1.6283913e+01 -8.4253387e+00]]\n",
      "indice: [0]\n",
      "Intent Identified:  despedida\n",
      "Response : Hasta pronto\n"
     ]
    }
   ],
   "source": [
    "get_response(\"no los conozco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00072238571"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-7.2238571e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00082370228"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-8.2370228e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00021944022"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.1944022e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
